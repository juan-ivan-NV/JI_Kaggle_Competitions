{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optiver_Realized_Volatility_Prediction_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwTGgAoPZOWP"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('max_columns', 300)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WdWXYNKa5j5"
      },
      "source": [
        "# pat\n",
        "data_dir = 'drive/MyDrive/optiver-realized-volatility-prediction/'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k74q0VTqb8Wq"
      },
      "source": [
        "# Function to calculate first WAP\n",
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "\n",
        "# Function to calculate second WAP\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pehSjc_Zb--6"
      },
      "source": [
        "# Function to calculate the log of the return\n",
        "# Remember that logb(x / y) = logb(x) - logb(y)\n",
        "def log_return(series):\n",
        "    return np.log(series).diff()\n",
        "\n",
        "# Calculate the realized volatility\n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "\n",
        "# Function to count unique elements of a series\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nog5SLDXcH3X"
      },
      "source": [
        "# Function to read our base train and test set\n",
        "def read_train_test():\n",
        "    \n",
        "    train = pd.read_csv('drive/MyDrive/optiver-realized-volatility-prediction/train.csv')\n",
        "    test = pd.read_csv('drive/MyDrive/optiver-realized-volatility-prediction/test.csv')\n",
        "    \n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    \n",
        "    return train, test"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qevAVOUidLAI"
      },
      "source": [
        "# Function to preprocess book data (for each stock id)\n",
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # Calculate Wap\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    # Calculate log returns\n",
        "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
        "    # Calculate wap balance\n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    # Calculate spread\n",
        "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.mean, np.std],\n",
        "        'wap2': [np.sum, np.mean, np.std],\n",
        "        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n",
        "        'wap_balance': [np.sum, np.mean, np.std],\n",
        "        'price_spread':[np.sum, np.mean, np.std],\n",
        "        'bid_spread':[np.sum, np.mean, np.std],\n",
        "        'ask_spread':[np.sum, np.mean, np.std],\n",
        "        'total_volume':[np.sum, np.mean, np.std],\n",
        "        'volume_imbalance':[np.sum, np.mean, np.std]\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "    \n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
        "    \n",
        "    # Create row_id so we can merge\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XWG5ZGrdLG4"
      },
      "source": [
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "    \n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum],\n",
        "        'order_count':[np.mean],\n",
        "    }\n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n",
        "    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n",
        "    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n",
        "    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n",
        "\n",
        "    # Merge all\n",
        "    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n",
        "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n",
        "    \n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co0tBXfsdLR2"
      },
      "source": [
        "# Function to get group stats for the stock_id and time_id\n",
        "def get_time_stock(df):\n",
        "    # Get realized volatility columns\n",
        "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n",
        "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n",
        "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
        "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "    \n",
        "    # Merge with original dataframe\n",
        "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
        "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
        "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
        "    return df"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp835NyAdk5L"
      },
      "source": [
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "\n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRR0ZX_Wdk_P"
      },
      "source": [
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US_y1vUodlE0"
      },
      "source": [
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLUqgz3zdlKW"
      },
      "source": [
        "def train_and_evaluate(train, test):\n",
        "    # Hyperparammeters (optimized)\n",
        "    seed = 42\n",
        "    params = {\n",
        "        'learning_rate': 0.13572437900113307,        \n",
        "        'lambda_l1': 2.154360665259325,\n",
        "        'lambda_l2': 6.711089761523827,\n",
        "        'num_leaves': 769,\n",
        "        'min_sum_hessian_in_leaf': 20.44437160769411,\n",
        "        'feature_fraction': 0.7921473067441019,\n",
        "        'feature_fraction_bynode': 0.8083803860191322,\n",
        "        'bagging_fraction': 0.9726755660563261,\n",
        "        'bagging_freq': 42,\n",
        "        'min_data_in_leaf': 690,\n",
        "        'max_depth': 3,\n",
        "        'seed': seed,\n",
        "        'feature_fraction_seed': seed,\n",
        "        'bagging_seed': seed,\n",
        "        'drop_seed': seed,\n",
        "        'data_random_seed': seed,\n",
        "        'objective': 'rmse',\n",
        "        'boosting': 'gbdt',\n",
        "        'verbosity': -1,\n",
        "        'n_jobs': -1,\n",
        "    }   \n",
        "    \n",
        "    # Split features and target\n",
        "    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
        "    y = train['target']\n",
        "    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
        "    # Transform stock id to a numeric value\n",
        "    x['stock_id'] = x['stock_id'].astype(int)\n",
        "    x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
        "    \n",
        "    # Create out of folds array\n",
        "    oof_predictions = np.zeros(x.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(x_test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n",
        "    # Iterate through each fold\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        # Root mean squared percentage error weights\n",
        "        ### model\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
        "        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
        "        model = lgb.train(params = params, \n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          num_boost_round = 10000, \n",
        "                          early_stopping_rounds = 67, \n",
        "                          verbose_eval = 200,\n",
        "                          feval = feval_rmspe)\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val)\n",
        "        # Predict the test set\n",
        "        test_predictions += model.predict(x_test) / 5\n",
        "        \n",
        "    rmspe_score = rmspe(y, oof_predictions)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "    # Return test predictions\n",
        "    return test_predictions"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZB0ClMEdlQS",
        "outputId": "ddd17684-4fa7-4fe7-9dfb-8ddd05cc4c62"
      },
      "source": [
        "# Read train and test\n",
        "train, test = read_train_test()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our training set has 428932 rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0fWRR_VeKPS",
        "outputId": "a4d63120-075a-42c0-e1b2-a2647df9b4fd"
      },
      "source": [
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on = ['row_id'], how = 'left')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 16.0min\n",
            "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 38.6min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCGvpS-CeKU1",
        "outputId": "4c1bd8c2-5eef-43bb-eca4-43d2a55c51ef"
      },
      "source": [
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    2.3s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGI2LhogeKaO"
      },
      "source": [
        "# Get group stats of time_id and stock_id\n",
        "train = get_time_stock(train)\n",
        "test = get_time_stock(test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si4SfuAVeKfp",
        "outputId": "4b3d89c0-77fe-4c5c-bcbf-173b48d5924f"
      },
      "source": [
        "# Traing and evaluate\n",
        "test_predictions = train_and_evaluate(train, test)\n",
        "# Save test predictions\n",
        "test['target'] = test_predictions\n",
        "test[['row_id', 'target']].to_csv('submission.csv',index = False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training fold 1\n",
            "Training until validation scores don't improve for 67 rounds.\n",
            "[200]\ttraining's rmse: 0.000465188\ttraining's RMSPE: 0.215455\tvalid_1's rmse: 0.000480881\tvalid_1's RMSPE: 0.221895\n",
            "[400]\ttraining's rmse: 0.000447752\ttraining's RMSPE: 0.207379\tvalid_1's rmse: 0.000465828\tvalid_1's RMSPE: 0.214949\n",
            "[600]\ttraining's rmse: 0.000434864\ttraining's RMSPE: 0.20141\tvalid_1's rmse: 0.00045581\tvalid_1's RMSPE: 0.210327\n",
            "[800]\ttraining's rmse: 0.000425144\ttraining's RMSPE: 0.196908\tvalid_1's rmse: 0.000449149\tvalid_1's RMSPE: 0.207253\n",
            "[1000]\ttraining's rmse: 0.000417\ttraining's RMSPE: 0.193136\tvalid_1's rmse: 0.000443779\tvalid_1's RMSPE: 0.204775\n",
            "[1200]\ttraining's rmse: 0.000410933\ttraining's RMSPE: 0.190326\tvalid_1's rmse: 0.000440266\tvalid_1's RMSPE: 0.203154\n",
            "[1400]\ttraining's rmse: 0.000405035\ttraining's RMSPE: 0.187595\tvalid_1's rmse: 0.000436693\tvalid_1's RMSPE: 0.201505\n",
            "[1600]\ttraining's rmse: 0.000400042\ttraining's RMSPE: 0.185282\tvalid_1's rmse: 0.000434509\tvalid_1's RMSPE: 0.200497\n",
            "[1800]\ttraining's rmse: 0.000395524\ttraining's RMSPE: 0.18319\tvalid_1's rmse: 0.000432059\tvalid_1's RMSPE: 0.199367\n",
            "[2000]\ttraining's rmse: 0.000391567\ttraining's RMSPE: 0.181357\tvalid_1's rmse: 0.000430537\tvalid_1's RMSPE: 0.198665\n",
            "[2200]\ttraining's rmse: 0.000387975\ttraining's RMSPE: 0.179693\tvalid_1's rmse: 0.000429527\tvalid_1's RMSPE: 0.198199\n",
            "[2400]\ttraining's rmse: 0.000384312\ttraining's RMSPE: 0.177997\tvalid_1's rmse: 0.000427963\tvalid_1's RMSPE: 0.197477\n",
            "[2600]\ttraining's rmse: 0.00038121\ttraining's RMSPE: 0.17656\tvalid_1's rmse: 0.000427162\tvalid_1's RMSPE: 0.197107\n",
            "[2800]\ttraining's rmse: 0.000378202\ttraining's RMSPE: 0.175167\tvalid_1's rmse: 0.000425898\tvalid_1's RMSPE: 0.196524\n",
            "[3000]\ttraining's rmse: 0.000375401\ttraining's RMSPE: 0.17387\tvalid_1's rmse: 0.000425127\tvalid_1's RMSPE: 0.196168\n",
            "[3200]\ttraining's rmse: 0.000372841\ttraining's RMSPE: 0.172684\tvalid_1's rmse: 0.000424588\tvalid_1's RMSPE: 0.19592\n",
            "[3400]\ttraining's rmse: 0.000370299\ttraining's RMSPE: 0.171507\tvalid_1's rmse: 0.000423801\tvalid_1's RMSPE: 0.195556\n",
            "Early stopping, best iteration is:\n",
            "[3428]\ttraining's rmse: 0.000369932\ttraining's RMSPE: 0.171337\tvalid_1's rmse: 0.000423672\tvalid_1's RMSPE: 0.195497\n",
            "Training fold 2\n",
            "Training until validation scores don't improve for 67 rounds.\n",
            "[200]\ttraining's rmse: 0.000465951\ttraining's RMSPE: 0.215534\tvalid_1's rmse: 0.000471659\tvalid_1's RMSPE: 0.218752\n",
            "[400]\ttraining's rmse: 0.000447935\ttraining's RMSPE: 0.2072\tvalid_1's rmse: 0.000457339\tvalid_1's RMSPE: 0.212111\n",
            "[600]\ttraining's rmse: 0.000435247\ttraining's RMSPE: 0.201331\tvalid_1's rmse: 0.000447796\tvalid_1's RMSPE: 0.207684\n",
            "[800]\ttraining's rmse: 0.00042566\ttraining's RMSPE: 0.196897\tvalid_1's rmse: 0.000440857\tvalid_1's RMSPE: 0.204466\n",
            "[1000]\ttraining's rmse: 0.000418061\ttraining's RMSPE: 0.193381\tvalid_1's rmse: 0.000436225\tvalid_1's RMSPE: 0.202318\n",
            "[1200]\ttraining's rmse: 0.000411214\ttraining's RMSPE: 0.190214\tvalid_1's rmse: 0.0004318\tvalid_1's RMSPE: 0.200266\n",
            "[1400]\ttraining's rmse: 0.000405416\ttraining's RMSPE: 0.187532\tvalid_1's rmse: 0.000428337\tvalid_1's RMSPE: 0.19866\n",
            "[1600]\ttraining's rmse: 0.000400416\ttraining's RMSPE: 0.185219\tvalid_1's rmse: 0.000426142\tvalid_1's RMSPE: 0.197641\n",
            "[1800]\ttraining's rmse: 0.000396175\ttraining's RMSPE: 0.183258\tvalid_1's rmse: 0.000424515\tvalid_1's RMSPE: 0.196887\n",
            "[2000]\ttraining's rmse: 0.000392224\ttraining's RMSPE: 0.18143\tvalid_1's rmse: 0.00042276\tvalid_1's RMSPE: 0.196073\n",
            "[2200]\ttraining's rmse: 0.000388841\ttraining's RMSPE: 0.179865\tvalid_1's rmse: 0.000421782\tvalid_1's RMSPE: 0.195619\n",
            "[2400]\ttraining's rmse: 0.000385391\ttraining's RMSPE: 0.178269\tvalid_1's rmse: 0.000420551\tvalid_1's RMSPE: 0.195048\n",
            "[2600]\ttraining's rmse: 0.000382002\ttraining's RMSPE: 0.176701\tvalid_1's rmse: 0.000419616\tvalid_1's RMSPE: 0.194615\n",
            "[2800]\ttraining's rmse: 0.000378928\ttraining's RMSPE: 0.17528\tvalid_1's rmse: 0.000419087\tvalid_1's RMSPE: 0.19437\n",
            "[3000]\ttraining's rmse: 0.000376257\ttraining's RMSPE: 0.174044\tvalid_1's rmse: 0.000418491\tvalid_1's RMSPE: 0.194093\n",
            "[3200]\ttraining's rmse: 0.000373709\ttraining's RMSPE: 0.172865\tvalid_1's rmse: 0.000417939\tvalid_1's RMSPE: 0.193837\n",
            "[3400]\ttraining's rmse: 0.000371135\ttraining's RMSPE: 0.171675\tvalid_1's rmse: 0.000417435\tvalid_1's RMSPE: 0.193603\n",
            "Early stopping, best iteration is:\n",
            "[3399]\ttraining's rmse: 0.00037115\ttraining's RMSPE: 0.171682\tvalid_1's rmse: 0.000417423\tvalid_1's RMSPE: 0.193598\n",
            "Training fold 3\n",
            "Training until validation scores don't improve for 67 rounds.\n",
            "[200]\ttraining's rmse: 0.000465232\ttraining's RMSPE: 0.215333\tvalid_1's rmse: 0.000471994\tvalid_1's RMSPE: 0.218375\n",
            "[400]\ttraining's rmse: 0.000447675\ttraining's RMSPE: 0.207206\tvalid_1's rmse: 0.000458515\tvalid_1's RMSPE: 0.212138\n",
            "[600]\ttraining's rmse: 0.000435524\ttraining's RMSPE: 0.201582\tvalid_1's rmse: 0.000450042\tvalid_1's RMSPE: 0.208218\n",
            "[800]\ttraining's rmse: 0.000425941\ttraining's RMSPE: 0.197147\tvalid_1's rmse: 0.000444109\tvalid_1's RMSPE: 0.205473\n",
            "[1000]\ttraining's rmse: 0.000418469\ttraining's RMSPE: 0.193688\tvalid_1's rmse: 0.000439544\tvalid_1's RMSPE: 0.203361\n",
            "[1200]\ttraining's rmse: 0.000411499\ttraining's RMSPE: 0.190462\tvalid_1's rmse: 0.000435553\tvalid_1's RMSPE: 0.201515\n",
            "[1400]\ttraining's rmse: 0.000405894\ttraining's RMSPE: 0.187868\tvalid_1's rmse: 0.000432715\tvalid_1's RMSPE: 0.200202\n",
            "[1600]\ttraining's rmse: 0.000400642\ttraining's RMSPE: 0.185437\tvalid_1's rmse: 0.000430301\tvalid_1's RMSPE: 0.199085\n",
            "[1800]\ttraining's rmse: 0.000396354\ttraining's RMSPE: 0.183452\tvalid_1's rmse: 0.000428848\tvalid_1's RMSPE: 0.198412\n",
            "[2000]\ttraining's rmse: 0.000392315\ttraining's RMSPE: 0.181583\tvalid_1's rmse: 0.000427546\tvalid_1's RMSPE: 0.19781\n",
            "[2200]\ttraining's rmse: 0.000388633\ttraining's RMSPE: 0.179879\tvalid_1's rmse: 0.000426146\tvalid_1's RMSPE: 0.197162\n",
            "[2400]\ttraining's rmse: 0.000385281\ttraining's RMSPE: 0.178327\tvalid_1's rmse: 0.000425263\tvalid_1's RMSPE: 0.196754\n",
            "[2600]\ttraining's rmse: 0.000381759\ttraining's RMSPE: 0.176697\tvalid_1's rmse: 0.000423853\tvalid_1's RMSPE: 0.196102\n",
            "[2800]\ttraining's rmse: 0.000379145\ttraining's RMSPE: 0.175487\tvalid_1's rmse: 0.00042347\tvalid_1's RMSPE: 0.195924\n",
            "[3000]\ttraining's rmse: 0.00037629\ttraining's RMSPE: 0.174166\tvalid_1's rmse: 0.000422904\tvalid_1's RMSPE: 0.195663\n",
            "[3200]\ttraining's rmse: 0.000373666\ttraining's RMSPE: 0.172951\tvalid_1's rmse: 0.000422131\tvalid_1's RMSPE: 0.195305\n",
            "[3400]\ttraining's rmse: 0.000371212\ttraining's RMSPE: 0.171815\tvalid_1's rmse: 0.00042196\tvalid_1's RMSPE: 0.195226\n",
            "Early stopping, best iteration is:\n",
            "[3366]\ttraining's rmse: 0.000371608\ttraining's RMSPE: 0.171999\tvalid_1's rmse: 0.000421856\tvalid_1's RMSPE: 0.195177\n",
            "Training fold 4\n",
            "Training until validation scores don't improve for 67 rounds.\n",
            "[200]\ttraining's rmse: 0.000464941\ttraining's RMSPE: 0.214854\tvalid_1's rmse: 0.000487833\tvalid_1's RMSPE: 0.227141\n",
            "[400]\ttraining's rmse: 0.000446358\ttraining's RMSPE: 0.206267\tvalid_1's rmse: 0.00047397\tvalid_1's RMSPE: 0.220686\n",
            "[600]\ttraining's rmse: 0.000433873\ttraining's RMSPE: 0.200497\tvalid_1's rmse: 0.000465667\tvalid_1's RMSPE: 0.21682\n",
            "[800]\ttraining's rmse: 0.000424361\ttraining's RMSPE: 0.196102\tvalid_1's rmse: 0.000459961\tvalid_1's RMSPE: 0.214163\n",
            "[1000]\ttraining's rmse: 0.000417095\ttraining's RMSPE: 0.192744\tvalid_1's rmse: 0.000455785\tvalid_1's RMSPE: 0.212219\n",
            "[1200]\ttraining's rmse: 0.000410633\ttraining's RMSPE: 0.189758\tvalid_1's rmse: 0.000451523\tvalid_1's RMSPE: 0.210234\n",
            "[1400]\ttraining's rmse: 0.000404874\ttraining's RMSPE: 0.187096\tvalid_1's rmse: 0.000448192\tvalid_1's RMSPE: 0.208683\n",
            "[1600]\ttraining's rmse: 0.000399838\ttraining's RMSPE: 0.184769\tvalid_1's rmse: 0.00044535\tvalid_1's RMSPE: 0.20736\n",
            "[1800]\ttraining's rmse: 0.0003955\ttraining's RMSPE: 0.182765\tvalid_1's rmse: 0.000443585\tvalid_1's RMSPE: 0.206538\n",
            "[2000]\ttraining's rmse: 0.000391303\ttraining's RMSPE: 0.180825\tvalid_1's rmse: 0.00044117\tvalid_1's RMSPE: 0.205414\n",
            "[2200]\ttraining's rmse: 0.000387378\ttraining's RMSPE: 0.179011\tvalid_1's rmse: 0.000438899\tvalid_1's RMSPE: 0.204356\n",
            "[2400]\ttraining's rmse: 0.000383906\ttraining's RMSPE: 0.177407\tvalid_1's rmse: 0.000437861\tvalid_1's RMSPE: 0.203873\n",
            "Early stopping, best iteration is:\n",
            "[2369]\ttraining's rmse: 0.000384386\ttraining's RMSPE: 0.177629\tvalid_1's rmse: 0.000437537\tvalid_1's RMSPE: 0.203722\n",
            "Training fold 5\n",
            "Training until validation scores don't improve for 67 rounds.\n",
            "[200]\ttraining's rmse: 0.000465292\ttraining's RMSPE: 0.215606\tvalid_1's rmse: 0.000478312\tvalid_1's RMSPE: 0.220284\n",
            "[400]\ttraining's rmse: 0.00044702\ttraining's RMSPE: 0.207139\tvalid_1's rmse: 0.000466201\tvalid_1's RMSPE: 0.214706\n",
            "[600]\ttraining's rmse: 0.000434441\ttraining's RMSPE: 0.201311\tvalid_1's rmse: 0.000456143\tvalid_1's RMSPE: 0.210074\n",
            "[800]\ttraining's rmse: 0.000424594\ttraining's RMSPE: 0.196748\tvalid_1's rmse: 0.000449541\tvalid_1's RMSPE: 0.207034\n",
            "[1000]\ttraining's rmse: 0.000416945\ttraining's RMSPE: 0.193203\tvalid_1's rmse: 0.000445018\tvalid_1's RMSPE: 0.20495\n",
            "[1200]\ttraining's rmse: 0.000410177\ttraining's RMSPE: 0.190067\tvalid_1's rmse: 0.000440977\tvalid_1's RMSPE: 0.203089\n",
            "[1400]\ttraining's rmse: 0.000404491\ttraining's RMSPE: 0.187432\tvalid_1's rmse: 0.000437699\tvalid_1's RMSPE: 0.20158\n",
            "[1600]\ttraining's rmse: 0.000399877\ttraining's RMSPE: 0.185294\tvalid_1's rmse: 0.00043574\tvalid_1's RMSPE: 0.200677\n",
            "[1800]\ttraining's rmse: 0.000395459\ttraining's RMSPE: 0.183247\tvalid_1's rmse: 0.00043333\tvalid_1's RMSPE: 0.199567\n",
            "[2000]\ttraining's rmse: 0.000391065\ttraining's RMSPE: 0.181211\tvalid_1's rmse: 0.0004311\tvalid_1's RMSPE: 0.198541\n",
            "[2200]\ttraining's rmse: 0.000387507\ttraining's RMSPE: 0.179562\tvalid_1's rmse: 0.00042971\tvalid_1's RMSPE: 0.1979\n",
            "[2400]\ttraining's rmse: 0.000384114\ttraining's RMSPE: 0.17799\tvalid_1's rmse: 0.000428782\tvalid_1's RMSPE: 0.197473\n",
            "[2600]\ttraining's rmse: 0.000380912\ttraining's RMSPE: 0.176506\tvalid_1's rmse: 0.000427665\tvalid_1's RMSPE: 0.196959\n",
            "[2800]\ttraining's rmse: 0.000378074\ttraining's RMSPE: 0.175191\tvalid_1's rmse: 0.000426899\tvalid_1's RMSPE: 0.196606\n",
            "[3000]\ttraining's rmse: 0.00037541\ttraining's RMSPE: 0.173957\tvalid_1's rmse: 0.000426365\tvalid_1's RMSPE: 0.19636\n",
            "[3200]\ttraining's rmse: 0.000372972\ttraining's RMSPE: 0.172827\tvalid_1's rmse: 0.000426043\tvalid_1's RMSPE: 0.196212\n",
            "[3400]\ttraining's rmse: 0.000370425\ttraining's RMSPE: 0.171647\tvalid_1's rmse: 0.000425713\tvalid_1's RMSPE: 0.196059\n",
            "Early stopping, best iteration is:\n",
            "[3530]\ttraining's rmse: 0.000368756\ttraining's RMSPE: 0.170873\tvalid_1's rmse: 0.000425467\tvalid_1's RMSPE: 0.195946\n",
            "Our out of folds RMSPE is 0.19682021257850602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyuorC4QeKkf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}